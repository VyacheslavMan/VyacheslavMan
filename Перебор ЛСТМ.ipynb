{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1141,
     "status": "ok",
     "timestamp": 1641026265024,
     "user": {
      "displayName": "Slava Manevich",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03367332962160017370"
     },
     "user_tz": -180
    },
    "id": "Ula51kGaugLr",
    "outputId": "f2129da5-7727-4140-c681-7a5da4ce0998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sheet1']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file='C:/Users/VIP13/Downloads/Все по нейронкам/all_rv.xlsx'\n",
    "\n",
    "# Load spreadsheet\n",
    "xl = pd.ExcelFile(file)\n",
    "\n",
    "# Print the sheet names\n",
    "print(xl.sheet_names)\n",
    "df1 = xl.parse(xl.sheet_names[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1640887747672,
     "user": {
      "displayName": "Slava Manevich",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03367332962160017370"
     },
     "user_tz": -180
    },
    "id": "FvC2IKPtvwbW",
    "outputId": "e278c15c-5fd4-4f3f-f7c0-190261b719d6"
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1431,
     "status": "ok",
     "timestamp": 1641026268903,
     "user": {
      "displayName": "Slava Manevich",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03367332962160017370"
     },
     "user_tz": -180
    },
    "id": "gNqy6UcR1N1o"
   },
   "outputs": [],
   "source": [
    "# вгрузил данные как надо, разбил на тест и трейн, отнормировал\n",
    "from sklearn import preprocessing\n",
    "# uni_data = df1['ln_rv_btc'].copy()\n",
    "\n",
    "uni_data = np.log(df1['rv_shk_x'].copy())\n",
    "uni_data.index = df1['Unnamed: 0']\n",
    "uni_data.head()\n",
    "xData=np.array(uni_data)\n",
    "xData = xData.reshape(len(xData),1)\n",
    "dataset=xData\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler() \n",
    "\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "#разбил на трен и тест\n",
    "coef_train_test=0.7\n",
    "\n",
    "train_size = int(len(dataset) * coef_train_test)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# разбил трен на трен и вал\n",
    "# coef_train_val=0.9\n",
    "\n",
    "# train_size = int(len(train) * coef_train_val)\n",
    "# val_size = len(train) - train_size\n",
    "\n",
    "# train1, val= train[0:val_size,:], train[train_size:len(dataset),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1641026271111,
     "user": {
      "displayName": "Slava Manevich",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03367332962160017370"
     },
     "user_tz": -180
    },
    "id": "OcQInsWmlAFD"
   },
   "outputs": [],
   "source": [
    "# univariate data preparation\n",
    "from numpy import array\n",
    " \n",
    "# split a univariate sequence into samples\n",
    "#здесь sequence - это выборка, которую разбиваем, n_steps - то самое \"скользящее окно\"\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    " \n",
    "# # define input sequence\n",
    "# raw_seq = train\n",
    "# # choose a number of time steps\n",
    "# n_steps = 250\n",
    "# # split into samples\n",
    "# X, y = split_sequence(raw_seq, n_steps)\n",
    "# # summarize the data\n",
    "# # for i in range(len(X)):\n",
    "# # \tprint(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1641026273423,
     "user": {
      "displayName": "Slava Manevich",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03367332962160017370"
     },
     "user_tz": -180
    },
    "id": "yR5R3ogKmQR_",
    "outputId": "f2b89b40-06bb-4411-9fa6-4489eddbd0b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(771, 100, 1)\n",
      "(771, 1)\n",
      "(274, 1)\n",
      "(274, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "n_steps = 100#хорошо работает при 250\n",
    "\n",
    "x_train, y_train=split_sequence(train, n_steps )\n",
    "x_test, y_test=split_sequence(test, n_steps)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "x_train_all=x_train.copy()\n",
    "y_train_all=y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(693, 100, 1)\n",
      "(78, 100, 1)\n",
      "(693, 1)\n",
      "(78, 1)\n"
     ]
    }
   ],
   "source": [
    "# разбил трен на трен и вал\n",
    "coef_train_val=0.9\n",
    "len_train=int(len(x_train)*coef_train_val)\n",
    "x_train1=x_train[0:len_train].copy()\n",
    "x_val=x_train[len_train:]\n",
    "x_train=x_train1\n",
    "\n",
    "y_train1=y_train[0:int(len(y_train)*coef_train_val)].copy()\n",
    "y_val=y_train[int(len(y_train)*coef_train_val):]\n",
    "y_train=y_train1\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PeUNEk3CCNeQ"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# def me(y_true, y_pred):\n",
    "def myloss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten, Convolution1D\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation, GlobalAveragePooling1D\n",
    "from keras.layers import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ТУТ заметки кода, можно вставлять кусками\n",
    "\n",
    "# СНН\n",
    "# model_m.add(Conv1D(5, 1, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "# modelC.add(MaxPooling1D(pool_size=1))\n",
    "\n",
    "# просто слой с числом нейронок и активатором\n",
    "# model_m.add(Dense(50, activation='linear'))\n",
    "# дропаут, для борьбы с переобучением\n",
    "# model.add(Dropout(0.25))\n",
    "# батчевая нормализация\n",
    "# model_m.add(BatchNormalization())\n",
    "# полносвязный слой, его лучше в конце вставлять\n",
    "# model_m.add(Flatten())\n",
    "# слой с регуляризацией\n",
    "# model_m.add(Dense(64, input_dim=64, bias_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01)))\n",
    "# раняя остановка, чтобы не переобучаться, в мониторе писать пример \n",
    "# early_stop = EarlyStopping(monitor = 'val_loss', patience = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz47DZ0QmY-R"
   },
   "source": [
    "сравнение пошло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    },
    "executionInfo": {
     "elapsed": 8947,
     "status": "ok",
     "timestamp": 1641026588044,
     "user": {
      "displayName": "Slava Manevich",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03367332962160017370"
     },
     "user_tz": -180
    },
    "id": "s80tCytLptlg",
    "outputId": "6cf26216-6f72-45ef-a1d9-7a94715ad1fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "47/47 [==============================] - 8s 87ms/step - loss: 0.0326 - val_loss: 0.0099\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 3s 73ms/step - loss: 0.0128 - val_loss: 0.0098\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 3s 68ms/step - loss: 0.0121 - val_loss: 0.0095\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 3s 67ms/step - loss: 0.0116 - val_loss: 0.0092\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 3s 65ms/step - loss: 0.0112 - val_loss: 0.0088\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 3s 65ms/step - loss: 0.0109 - val_loss: 0.0085\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 3s 67ms/step - loss: 0.0106 - val_loss: 0.0083\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0105 - val_loss: 0.0082\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0103 - val_loss: 0.0080\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 3s 65ms/step - loss: 0.0102 - val_loss: 0.0079\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0101 - val_loss: 0.0077\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 3s 65ms/step - loss: 0.0100 - val_loss: 0.0076\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0100 - val_loss: 0.0075\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 3s 73ms/step - loss: 0.0099 - val_loss: 0.0074\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0098 - val_loss: 0.0074\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 4s 80ms/step - loss: 0.0098 - val_loss: 0.0073\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 3s 69ms/step - loss: 0.0097 - val_loss: 0.0072\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 3s 72ms/step - loss: 0.0097 - val_loss: 0.0072\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 4s 77ms/step - loss: 0.0097 - val_loss: 0.0071\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 4s 79ms/step - loss: 0.0096 - val_loss: 0.0071\n",
      "0\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 9s 91ms/step - loss: 0.0287 - val_loss: 0.0100\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 3s 69ms/step - loss: 0.0134 - val_loss: 0.0103\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 3s 69ms/step - loss: 0.0130 - val_loss: 0.0101\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 3s 67ms/step - loss: 0.0125 - val_loss: 0.0098\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0121 - val_loss: 0.0095\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 3s 69ms/step - loss: 0.0117 - val_loss: 0.0092\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0114 - val_loss: 0.0090\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0112 - val_loss: 0.0089\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 3s 65ms/step - loss: 0.0110 - val_loss: 0.0087\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 3s 73ms/step - loss: 0.0108 - val_loss: 0.0085\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 3s 67ms/step - loss: 0.0106 - val_loss: 0.0083\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 3s 69ms/step - loss: 0.0105 - val_loss: 0.0082\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0104 - val_loss: 0.0080\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0102 - val_loss: 0.0079\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 3s 68ms/step - loss: 0.0101 - val_loss: 0.0078\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 3s 69ms/step - loss: 0.0101 - val_loss: 0.0076\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 3s 69ms/step - loss: 0.0100 - val_loss: 0.0075\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0099 - val_loss: 0.0075\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 3s 68ms/step - loss: 0.0099 - val_loss: 0.0074\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0098 - val_loss: 0.0073\n",
      "1\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 8s 96ms/step - loss: 0.0301 - val_loss: 0.0102\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0144 - val_loss: 0.0107\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 3s 74ms/step - loss: 0.0138 - val_loss: 0.0106\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0134 - val_loss: 0.0095\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 3s 69ms/step - loss: 0.0136 - val_loss: 0.0100\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0124 - val_loss: 0.0097\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0120 - val_loss: 0.0095\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 3s 68ms/step - loss: 0.0117 - val_loss: 0.0094\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 3s 68ms/step - loss: 0.0115 - val_loss: 0.0092\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 3s 69ms/step - loss: 0.0112 - val_loss: 0.0090\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0111 - val_loss: 0.0088\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0112 - val_loss: 0.0089\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0107 - val_loss: 0.0085\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0106 - val_loss: 0.0083\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0105 - val_loss: 0.0081\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0103 - val_loss: 0.0080\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0102 - val_loss: 0.0079\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 3s 68ms/step - loss: 0.0101 - val_loss: 0.0078\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 3s 67ms/step - loss: 0.0101 - val_loss: 0.0077\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 3s 69ms/step - loss: 0.0100 - val_loss: 0.0076\n",
      "2\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 8s 89ms/step - loss: 0.0577 - val_loss: 0.0201\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0201 - val_loss: 0.0174\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 3s 73ms/step - loss: 0.0198 - val_loss: 0.0161\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0195 - val_loss: 0.0158\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 3s 71ms/step - loss: 0.0194 - val_loss: 0.0155\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 3s 65ms/step - loss: 0.0194 - val_loss: 0.0153\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0193 - val_loss: 0.0150\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 3s 66ms/step - loss: 0.0192 - val_loss: 0.0149\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0192 - val_loss: 0.0147\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 3s 68ms/step - loss: 0.0191 - val_loss: 0.0146\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0190 - val_loss: 0.0143\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 3s 68ms/step - loss: 0.0190 - val_loss: 0.0142\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 3s 67ms/step - loss: 0.0190 - val_loss: 0.0141\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 3s 71ms/step - loss: 0.0190 - val_loss: 0.0141\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 3s 68ms/step - loss: 0.0189 - val_loss: 0.0140\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 3s 72ms/step - loss: 0.0189 - val_loss: 0.0140\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 3s 68ms/step - loss: 0.0189 - val_loss: 0.0140\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 3s 71ms/step - loss: 0.0189 - val_loss: 0.0139\n",
      "3\n",
      "Epoch 1/20\n",
      "47/47 [==============================] - 9s 91ms/step - loss: 0.0230 - val_loss: 0.0098\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 3s 72ms/step - loss: 0.0125 - val_loss: 0.0098\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 3s 69ms/step - loss: 0.0121 - val_loss: 0.0096\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 3s 74ms/step - loss: 0.0118 - val_loss: 0.0092\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0115 - val_loss: 0.0089\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 3s 73ms/step - loss: 0.0112 - val_loss: 0.0087\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 3s 74ms/step - loss: 0.0110 - val_loss: 0.0086\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 3s 72ms/step - loss: 0.0108 - val_loss: 0.0084\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 3s 73ms/step - loss: 0.0106 - val_loss: 0.0082\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 3s 73ms/step - loss: 0.0104 - val_loss: 0.0081\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0103 - val_loss: 0.0079\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 3s 73ms/step - loss: 0.0102 - val_loss: 0.0078\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0101 - val_loss: 0.0077\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 3s 72ms/step - loss: 0.0100 - val_loss: 0.0076\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 3s 71ms/step - loss: 0.0099 - val_loss: 0.0075\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 3s 71ms/step - loss: 0.0099 - val_loss: 0.0074\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 4s 75ms/step - loss: 0.0098 - val_loss: 0.0073\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 3s 72ms/step - loss: 0.0098 - val_loss: 0.0073\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 3s 71ms/step - loss: 0.0097 - val_loss: 0.0072\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 3s 70ms/step - loss: 0.0097 - val_loss: 0.0072\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from pandas import ExcelWriter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Bidirectional\n",
    "from keras.layers import Input, Flatten, Activation, Reshape, RepeatVector, Concatenate\n",
    "from keras.layers import Flatten, Convolution1D\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation, GlobalAveragePooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from keras import regularizers\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "activation_functions=['tanh', 'linear', 'relu', 'softmax', 'elu', 'selu', 'softplus', 'softsign', 'sigmoid', 'hard_sigmoid', 'exponential', 'LeakyReLU', 'PReLU', 'ThresholdedReLU']\n",
    "# activation_functions_n=['selu', 'softplus', 'softsign', 'sigmoid', 'hard_sigmoid', 'exponential', 'LeakyReLU', 'PReLU', 'ThresholdedReLU']\n",
    "\n",
    "# loss_f=['mae', 'mse']\n",
    "# T_F=['True', 'False']\n",
    "\n",
    "# all_logir=pd.DataFrame({ })\n",
    "i=0\n",
    "all_logir_pred=pd.DataFrame()\n",
    "all_logir_mae=pd.DataFrame()\n",
    "all_logir_mse=pd.DataFrame()\n",
    "all_logir_me=pd.DataFrame()\n",
    "\n",
    "testY = scaler.inverse_transform(y_test)\n",
    "testY=testY.reshape(testY.shape[0])\n",
    "all_logir_pred['testY']=testY\n",
    "\n",
    "name_cols=[]\n",
    "for a_1 in activation_functions:\n",
    "    for d_1 in np.arange(0, 0.3, 0.1):\n",
    "        for a_2 in activation_functions:\n",
    "            for d_2 in np.arange(0, 0.3, 0.1):\n",
    "                for a_3 in activation_functions:\n",
    "                                            \n",
    "                    i=i+1\n",
    "                    name_col=(str(a_1)+'-'+str(d_1)+'-'+\n",
    "                            str(a_2)+'-'+str(d_2)+'-'+str(a_3)+'-'+str(i))\n",
    "                    name_cols.append(name_col)\n",
    "                    \n",
    "                    \n",
    "# for i in range(0, len(name_cols)):\n",
    "for i in range(0, 5):\n",
    "\n",
    "    nn=name_cols[i]\n",
    "    nnn=nn.split('-')  \n",
    "    a_1=nnn[0]\n",
    "    d_1=float(nnn[1])\n",
    "    a_2=nnn[2]\n",
    "    d_2=float(nnn[3])\n",
    "    a_3=nnn[4]\n",
    "\n",
    "    \n",
    "    model_m = Sequential()                        \n",
    "    model_m.add(Dense(1, input_shape=(x_train.shape[1], x_train.shape[2])))#входной нейрон\n",
    "\n",
    "    model_m.add(LSTM(64,return_sequences = True, activation=a_1, input_shape = (x_train.shape[1], x_train.shape[2])))\n",
    "    model_m.add(Dropout(d_1))\n",
    "    model_m.add(LSTM(32, activation=a_2, input_shape = (x_train.shape[1], x_train.shape[2])))\n",
    "    model_m.add(Dropout(d_2))\n",
    "\n",
    "    model_m.add(Dense(10, activation=a_3))\n",
    "    model_m.add(Flatten())\n",
    "    model_m.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model_m.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    early_stop = EarlyStopping(monitor = 'loss', patience = 5)\n",
    "\n",
    "    history = model_m.fit(x_train, \n",
    "                                                            y_train, \n",
    "                                                            epochs=20, \n",
    "                                                            batch_size=15, #для борьбы с затухающим градиентом, как часто перетасовывать говну данных\n",
    "                                                            verbose=1,\n",
    "                                                            shuffle = False,\n",
    "                                                            callbacks = [early_stop],\n",
    "                                                            validation_data=(x_val, y_val)\n",
    "                                                            )\n",
    "    predicted = model_m.predict(x_test)\n",
    "    testPredict = scaler.inverse_transform(predicted)\n",
    "    testPredict=testPredict.reshape(testPredict.shape[0])\n",
    "    all_logir_pred[str(nn)]=testPredict\n",
    "\n",
    "# predicted.reshape(predicted.shape[0])\n",
    "    me=testY-testPredict\n",
    "    mae=abs(testY-testPredict)\n",
    "    mse=(testY-testPredict)**2\n",
    "                            \n",
    "                \n",
    "#                     testY.reshape(testY.shape[0])\n",
    "    all_logir_me[str(nn)]=me.reshape(me.shape[0])\n",
    "    all_logir_mae[str(nn)]=mae.reshape(mae.shape[0])\n",
    "    all_logir_mse[str(nn)]=mse.reshape(mse.shape[0])\n",
    "                    \n",
    "    if i % 2 == 0:\n",
    "        writer_pred=pd.ExcelWriter('all_logir_pred.xlsx')\n",
    "        all_logir_pred.to_excel(writer_pred)\n",
    "        writer_pred.save()\n",
    "#                         writer_pred.close()\n",
    "                        \n",
    "        writer_me=pd.ExcelWriter('all_logir_me.xlsx')\n",
    "        all_logir_me.to_excel(writer_me)\n",
    "        writer_me.save()\n",
    "#                         writer_me.close()\n",
    "                        \n",
    "        writer_mae=pd.ExcelWriter('all_logir_mae.xlsx')\n",
    "        all_logir_mae.to_excel(writer_mae)\n",
    "        writer_mae.save()\n",
    "#                         writer_mae.close()\n",
    "                        \n",
    "        writer_mse=pd.ExcelWriter('all_logir_mse.xlsx')\n",
    "        all_logir_mse.to_excel(writer_mse)\n",
    "        writer_mse.save()\n",
    "#                         writer_mse.close()\n",
    "                        \n",
    "    print(i)\n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter\n",
    "\n",
    "print(i)\n",
    "writer_pred=pd.ExcelWriter('all_logir_pred.xlsx')\n",
    "all_logir_pred.to_excel(writer_pred)\n",
    "writer_pred.save()\n",
    "writer_pred.close()\n",
    "                        \n",
    "writer_me=pd.ExcelWriter('all_logir_me.xlsx')\n",
    "all_logir_me.to_excel(writer_me)\n",
    "writer_me.save()\n",
    "writer_me.close()\n",
    "                        \n",
    "writer_mae=pd.ExcelWriter('all_logir_mae.xlsx')\n",
    "all_logir_mae.to_excel(writer_mae)\n",
    "writer_mae.save()\n",
    "writer_mae.close()\n",
    "                        \n",
    "writer_mse=pd.ExcelWriter('all_logir_mse.xlsx')\n",
    "all_logir_mse.to_excel(writer_mse)\n",
    "writer_mse.save()\n",
    "writer_mse.close()\n",
    "\n",
    "end_time=time.time() - start_time    \n",
    "print(end_time)\n",
    "print(i)\n",
    "print(all_logir_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOU/PZRjWdHtZLJn/X5bccw",
   "collapsed_sections": [],
   "mount_file_id": "18rpzGo5XmWRKJ9Fsb_4BhlAKhWvA2Rjq",
   "name": "Еще одна нейронка.ipynb",
   "provenance": [
    {
     "file_id": "18rpzGo5XmWRKJ9Fsb_4BhlAKhWvA2Rjq",
     "timestamp": 1640938927736
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
